\subsection{Data Generation}
\label{sec:soft_cut_data_generation}

The raw data set from the 2007 TrecVid contest contains 1,592 soft cut frames and 637,755 non soft cut frames.
This is a highly imbalanced ratio, which impedes the training.
Also, the number of soft cuts in the data set is too small for training a deep neural net.
As stated by Greg Corrado, senior research scientist at Google: ``Training deep learning models really does take a lot of data''~\cite{dataDeepNeuralNets}.
Therefore, we generated more soft cuts by blending random sequences into each other.

The procedure for generating a new random sequence based on the data set provided by the 2007 TrecVid contest works as follows:
First, we randomly pick two subsequent cuts from the gold standard provided by the TrecVid contest, each with a start and end frame respectively.
Between the end frame of the first cut and the start frame of the next cut, we randomly select a subsequence with the desired transition length.
Because there is no hard or soft cut between two subsequent cuts in the gold standard, it is guaranteed that there is also no hard or soft cut in the random sequence we picked.
Afterwards, we repeat this process with two other random cuts.
As the result, we now have two randomly selected sequences of the same length, which we now blend into each other.
To blend two random sequences, we have multiple options for tweening behaviour.
The standard tweening function is linear, but we also used ease-in, ease-out, and others.
The tweening function is randomly selected, as well.
As the transition type we use a classic \textit{dissolve}.
To achieve further variance in the generated data, we flip the two sequences randomly at the x- or y-axis or at both axes.
With this approach we generated about 450,000 sequences of length 11 and length 21 each.
Length 11 was chosen as a first approach, because it is slightly larger than the minimum size of soft cuts.
The intuition was, that also the smallest soft cuts can be detected with this approach.
Length 21 was chosen, because it is the average soft cut size in the original data.
In each case 50\% of the the sequences are non cuts and 50\% are soft cuts.
This makes a total of 6,400,000 soft cut frames and 6,400,000 non cut frames.
